# ZanderMCP

Cloud-based MCP (Model Context Protocol) server for real-time BCI classification and cognitive state monitoring.

## Overview

ZanderMCP enables AI assistants (Claude, Continue, etc.) to access real-time brain-computer interface (BCI) data for adaptive behavior and research applications. The system consists of:

- **Edge Relay**: Lightweight local service that forwards LSL streams to the cloud
- **ZanderMCP Server**: Cloud-hosted FastMCP server with classification and persistence
- **Model Service**: Separate service for hosting ML classifiers (optional)
- **Database**: PostgreSQL + TimescaleDB for time-series data storage

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Local (with EEG device)      â”‚
â”‚  EEG Device â†’ LSL Stream    â”‚
â”‚        â†“                     â”‚
â”‚  Edge Relay (relay.py)      â”‚
â”‚   - Reads LSL               â”‚
â”‚   - Preprocesses (optional) â”‚
â”‚   - Sends to cloud          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Deployment             â”‚
â”‚  ZanderMCP Server           â”‚
â”‚   - WebSocket ingestion     â”‚
â”‚   - Classification          â”‚
â”‚   - MCP tools for AI        â”‚
â”‚   - Database persistence    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘ MCP Protocol
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Clients                   â”‚
â”‚  Claude Code, Claude Desktopâ”‚
â”‚  Continue, Custom MCP apps  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Features

### âœ… Implemented (Phase 1)

- **Database Layer**
  - PostgreSQL with TimescaleDB for time-series optimization
  - SQLAlchemy async ORM with connection pooling
  - Batched writes for performance
  - Alembic migrations for schema management

- **Signal Processing**
  - Ported from bci-direct project
  - Bandpass filtering (1-40 Hz, 4th order Butterworth)
  - Power spectral density computation
  - Band power extraction (delta, theta, alpha, beta, gamma)
  - Deterministic workload calculation

- **Classifiers**
  - Base classifier interface
  - Signal processing classifier (deterministic, no ML required)
  - ML classifier client for Azure-hosted models (planned)

- **Edge Relay**
  - LSL stream reading
  - WebSocket connection to cloud
  - Auto-reconnection with buffering
  - Optional local preprocessing
  - Compression support (msgpack/json)

### ğŸš§ In Progress (Phase 2)

- **ZanderMCP Server** (server.py)
  - FastMCP implementation
  - WebSocket server for edge relay
  - MCP tools for cognitive load queries
  - Session management
  - Real-time classification pipeline

- **ML Classifier Integration**
  - HTTP client for Azure-hosted models
  - Feature extraction and preprocessing
  - Fallback to signal processing classifier

- **Deployment**
  - Docker containers
  - docker-compose for local dev
  - Cloud deployment configs

## Project Structure

```
ZanderMCP/
â”œâ”€â”€ server.py                      # Main FastMCP server (TODO)
â”œâ”€â”€ edge_relay/
â”‚   â”œâ”€â”€ relay.py                   # Edge relay application âœ“
â”‚   â””â”€â”€ edge_relay_config.yaml     # Edge relay config âœ“
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ websocket_server.py        # WebSocket ingestion (TODO)
â”‚   â””â”€â”€ stream_buffer.py           # Real-time buffer (TODO)
â”œâ”€â”€ classifiers/
â”‚   â”œâ”€â”€ base.py                    # Base classifier âœ“
â”‚   â”œâ”€â”€ signal_processing.py       # Signal processing classifier âœ“
â”‚   â””â”€â”€ azure_ml.py                # Azure ML classifier client (TODO)
â”œâ”€â”€ signal_processing/
â”‚   â”œâ”€â”€ preprocessing.py           # Filtering, PSD âœ“
â”‚   â””â”€â”€ features.py                # Feature extraction âœ“
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ models.py                  # SQLAlchemy models âœ“
â”‚   â”œâ”€â”€ connection.py              # DB connection âœ“
â”‚   â””â”€â”€ persistence.py             # Batched writes âœ“
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ realtime.py                # Real-time MCP tools (TODO)
â”‚   â”œâ”€â”€ history.py                 # Historical query tools (TODO)
â”‚   â””â”€â”€ session.py                 # Session management (TODO)
â”œâ”€â”€ alembic/                        # Database migrations âœ“
â”œâ”€â”€ pyproject.toml                  # Dependencies âœ“
â”œâ”€â”€ config.yaml                     # Server configuration âœ“
â”œâ”€â”€ .env.example                    # Environment variables template âœ“
â””â”€â”€ README.md                       # This file âœ“
```

## Installation

### Prerequisites

- Python 3.12+
- PostgreSQL with TimescaleDB extension
- For edge relay: LSL-compatible EEG device

### Setup

1. **Clone and install dependencies:**

```bash
cd ZanderMCP
pip install -e .
# or with uv:
uv pip install -e .
```

2. **Configure environment:**

```bash
cp .env.example .env
# Edit .env with your database URL and API keys
```

3. **Initialize database:**

```bash
# Create database schema and TimescaleDB hypertables
alembic upgrade head
# or run initialization script:
python -c "import asyncio; from database.connection import init_database; asyncio.run(init_database())"
```

4. **Configure edge relay:**

Edit `edge_relay/edge_relay_config.yaml`:
- Set `lsl.stream_name` to your EEG device's LSL stream name
- Set `cloud.endpoint` to your ZanderMCP server URL
- Set `cloud.api_key` and `cloud.user_id`

## Usage

### Running Edge Relay (Local Machine)

```bash
cd edge_relay
python relay.py edge_relay_config.yaml
```

The edge relay will:
1. Connect to your local LSL stream
2. Connect to the cloud ZanderMCP server
3. Forward EEG data continuously
4. Auto-reconnect if connection is lost
5. Buffer data during disconnections

### Running ZanderMCP Server (Cloud)

```bash
# TODO: Once server.py is implemented
python server.py
# or with MCP dev mode:
mcp dev server.py
```

### Using with AI Clients

**Claude Code / Claude Desktop:**

Add to your MCP configuration:

```json
{
  "mcpServers": {
    "zandermcp": {
      "command": "python",
      "args": ["/path/to/ZanderMCP/server.py"]
    }
  }
}
```

**Example AI Assistant Usage:**

```
User: "How is my cognitive load right now?"

AI calls: get_current_cognitive_load()
Response: { workload: 0.73, confidence: 1.0, trend: "increasing" }

AI: "Your cognitive load is moderately high (0.73) and increasing.
     You might want to take a break soon."
```

## MCP Tools (Planned)

### Real-time Tools
- `get_current_cognitive_load()` - Latest workload prediction
- `get_cognitive_state()` - Current state with context
- `check_attention_level()` - Attention score

### Classifier Tools
- `list_classifiers()` - Available classifiers
- `switch_classifier(name)` - Change active classifier
- `get_classifier_info()` - Current classifier metadata

### Historical Tools
- `query_workload_history(minutes)` - Recent trends
- `get_session_summary()` - Session statistics
- `analyze_cognitive_patterns(start, end)` - Pattern analysis

### Research Tools
- `export_session_data(format)` - Download data
- `get_raw_features(timestamp)` - Raw features for debugging
- `annotate_event(timestamp, label, notes)` - Mark events

## Database Schema

### Tables

- **sessions**: BCI recording sessions
- **predictions**: Classification predictions (hypertable)
- **events**: User-annotated events
- **feature_vectors**: Extracted EEG features (optional)
- **stream_samples**: Raw stream data (optional, high volume)
- **model_predictions**: Model performance tracking

### Time-Series Optimization

Tables `predictions` and `stream_samples` use TimescaleDB hypertables for:
- Automatic partitioning by time
- Efficient time-range queries
- Data retention policies
- Continuous aggregates

## Configuration

### server.py Configuration (config.yaml)

See `config.yaml` for full configuration options:
- WebSocket server settings
- Signal processing parameters
- Classifier configuration
- Database persistence options
- Session management

### Edge Relay Configuration

See `edge_relay/edge_relay_config.yaml`:
- LSL stream name
- Cloud endpoint and auth
- Preprocessing options
- Buffer and compression settings

## Development

### Running Tests

```bash
pytest tests/
```

### Database Migrations

```bash
# Create a new migration
alembic revision --autogenerate -m "description"

# Apply migrations
alembic upgrade head

# Rollback
alembic downgrade -1
```

### Code Formatting

```bash
black .
ruff check --fix .
```

## Deployment

### Docker Deployment (TODO)

```bash
# Build and run with docker-compose
docker-compose up -d

# View logs
docker-compose logs -f zandermcp
```

### Cloud Platforms

- **Fly.io**: `fly launch` and `fly deploy`
- **Railway**: Connect GitHub repo, auto-deploy
- **AWS ECS**: Use provided Dockerfile
- **GCP Cloud Run**: `gcloud run deploy`

## Use Cases

### 1. Adaptive AI Assistants
AI assistants detect high cognitive load and:
- Simplify explanations
- Suggest breaks
- Adjust interaction complexity
- Provide encouragement

### 2. Research & Data Collection
- Continuous data logging
- Event annotation
- Session management
- Export for offline analysis

### 3. Neurofeedback Applications
- Real-time workload monitoring
- Training applications
- Performance optimization
- Attention training

### 4. BCI Control Systems
- Intent detection
- Command classification
- Adaptive interfaces
- Accessibility applications

## Roadmap

- [x] Phase 1: Database, signal processing, edge relay
- [ ] Phase 2: ZanderMCP server, MCP tools
- [ ] Phase 3: Model service, ML classifier support
- [ ] Phase 4: Docker deployment, CI/CD
- [ ] Phase 5: Advanced analytics, multi-user support
- [ ] Phase 6: Mobile edge relay app

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Submit a pull request

## License

[To be determined]

## Acknowledgments

- Signal processing code ported from [bci-direct](https://github.com/yourusername/bci-direct)
- Built with [FastMCP](https://github.com/jlowin/fastmcp)
- Uses [Lab Streaming Layer](https://labstreaminglayer.org/)

## Support

For issues, questions, or contributions:
- GitHub Issues: [repository URL]
- Documentation: [docs URL]
- Email: [contact email]
